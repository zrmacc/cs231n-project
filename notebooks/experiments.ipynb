{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow4sfyhNHvO1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o55_qqceH96-",
    "outputId": "bd186957-89ac-4206-b5bc-0e2d7372959b"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/cs231n\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW0Q0y1qcDaB"
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 400\n",
    "POS_DIR = \"data/pos_d1\"\n",
    "NEG_DIR = \"data/neg_d1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpTBf2cy2U31"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8DkIZQo2V8z"
   },
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    dir: str,\n",
    "    target_shape=[224, 224]\n",
    "  ) -> np.ndarray:\n",
    "  \"\"\"Load data.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    dir: Directory from which to load images.\n",
    "    target_shape: Shape for output\n",
    "\n",
    "  \"\"\"\n",
    "  orig_dir = os.getcwd()\n",
    "  os.chdir(dir)\n",
    "  files = os.listdir()\n",
    "  arrays = []\n",
    "  for in_file in files:\n",
    "    arrays.append(np.load(in_file, allow_pickle=True))\n",
    "  out = np.stack(arrays)\n",
    "  if target_shape != [256, 256]:\n",
    "    out = tf.image.resize(out, target_shape)\n",
    "    out = out.numpy()\n",
    "  os.chdir(orig_dir)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvW3aell6wCZ"
   },
   "source": [
    "## Training validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zA2CAdj8bc8"
   },
   "outputs": [],
   "source": [
    "def gen_sampling_weights(y: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Inverse probability sampling weights.\n",
    "  \n",
    "  Prepares a weight vector with weight inversely proportional to the\n",
    "  frequency of the class in the input vector.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    y: Binary [0, 1] labels.\n",
    "  \n",
    "  \"\"\"\n",
    "  n1 = sum(y == 1)\n",
    "  n0 = sum(y == 0)\n",
    "  p1 = n1 / (n1 + n0)\n",
    "  p0 = 1.0 - p1\n",
    "  w1 = 1.0 / p1\n",
    "  w0 = 1.0 / p0\n",
    "  out = np.ones_like(y)\n",
    "  out[y == 1] = w1\n",
    "  out[y == 0] = w0\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r987Hj8GDwro"
   },
   "outputs": [],
   "source": [
    "def prep_data_for_model(\n",
    "    pos: np.ndarray,\n",
    "    neg: np.ndarray,\n",
    "    train_prop = 0.7,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "  \"\"\"Prepare data for modeling.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    pos: Positive example images.\n",
    "    neg: Negative example images.\n",
    "    train_prop: Proportion of data for training. Remaining is allocated to\n",
    "    validation.\n",
    "  \n",
    "  \"\"\"\n",
    "  n1 = pos.shape[0]\n",
    "  n0 = neg.shape[0]\n",
    "  cut1 = int(train_prop * n1)\n",
    "  cut0 = int(train_prop * n0)\n",
    "\n",
    "  # Split into training and validation sets.\n",
    "  pos_train = pos[:cut1, :]\n",
    "  pos_val = pos[cut1:, :]\n",
    "  neg_train = neg[:cut0, :]\n",
    "  neg_val = neg[cut0:, :]\n",
    "  x_train = np.concatenate((pos_train, neg_train), axis=0)\n",
    "  x_val = np.concatenate((pos_val, neg_val), axis=0)\n",
    "\n",
    "  # Prepare labels.\n",
    "  y1_train = np.ones(pos_train.shape[0])\n",
    "  y0_train = np.zeros(neg_train.shape[0])\n",
    "  y_train = np.concatenate((y1_train, y0_train), axis=0)\n",
    "\n",
    "  y1_val = np.ones(pos_val.shape[0])\n",
    "  y0_val = np.zeros(neg_val.shape[0])\n",
    "  y_val = np.concatenate((y1_val, y0_val), axis=0)\n",
    "\n",
    "  # Prepare weights.\n",
    "  w_train = gen_sampling_weights(y_train)\n",
    "  w_val = gen_sampling_weights(y_val)\n",
    "\n",
    "  # Output.\n",
    "  return {\n",
    "      \"x_train\": x_train,\n",
    "      \"y_train\": y_train,\n",
    "      \"w_train\": w_train,\n",
    "      \"x_val\": x_val,\n",
    "      \"y_val\": y_val,\n",
    "      \"w_val\": w_val\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyQ4V1drIui_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfI79m9OIaA3"
   },
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjaZ9xYuGDgg"
   },
   "outputs": [],
   "source": [
    "def prep_generators(\n",
    "    data: Dict[str, np.ndarray],\n",
    "    train_batch=64,\n",
    "    val_batch=32,\n",
    ") -> Dict:\n",
    "  \"\"\"Prepare data generators\n",
    "\n",
    "  Generators apply random relections and rotations.\n",
    "\n",
    "  \"\"\"\n",
    "  transformer = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "  )\n",
    "\n",
    "  train_generator = transformer.flow(\n",
    "    x=data[\"x_train\"],\n",
    "    y=data[\"y_train\"],\n",
    "    batch_size=train_batch,\n",
    "    sample_weight=data[\"w_train\"]\n",
    "  )\n",
    "\n",
    "  val_generator = transformer.flow(\n",
    "    x=data[\"x_val\"],\n",
    "    y=data[\"y_val\"],\n",
    "    batch_size=val_batch,\n",
    "    sample_weight=data[\"w_val\"]\n",
    "  )\n",
    "\n",
    "  return {\n",
    "      \"train_gen\": train_generator,\n",
    "      \"val_gen\": val_generator\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT-Jt9eF9ZSY"
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDEzg6TDJCNF"
   },
   "outputs": [],
   "source": [
    "def _conv_layer(nodes, l2=0.0, name=\"conv\") -> tf.keras.layers.Layer:\n",
    "  return tf.keras.layers.Conv2D(\n",
    "        filters=nodes,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2),\n",
    "        name=name,\n",
    "        padding=\"same\",\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqVTKTQcVvAB"
   },
   "outputs": [],
   "source": [
    "def _custom_model(\n",
    "    conv_blocks=3,\n",
    "    drop_prob=0.0,\n",
    "    l2=0.0,\n",
    "    min_nodes=32,\n",
    "    max_nodes=256\n",
    ") -> tf.keras.Model:\n",
    "  \"\"\"Architecture of custom model.\"\"\"\n",
    "  # First convolutional block.\n",
    "  nodes = min_nodes\n",
    "  model = tf.keras.Sequential([\n",
    "    _conv_layer(nodes, l2=l2, name=\"1a\"),\n",
    "    _conv_layer(nodes, l2=l2, name=\"1b\")                          \n",
    "  ])\n",
    "  # Dropout.\n",
    "  if drop_prob > 0:\n",
    "    model.add(tf.keras.layers.Dropout(drop_prob))\n",
    "\n",
    "  # Subsequent convolutional blocks.\n",
    "  for i in range(1, conv_blocks):\n",
    "    nodes = min(nodes * 2, max_nodes)\n",
    "    model.add(tf.keras.layers.MaxPool2D(name=f\"max_pool{i}\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(name=f\"batch_norm{i}\"))\n",
    "    model.add(_conv_layer(nodes, l2=l2, name=f\"{i+1}a\"))\n",
    "    model.add(_conv_layer(nodes, l2=l2, name=f\"{i+1}b\"))\n",
    "    if drop_prob > 0:\n",
    "      model.add(tf.keras.layers.Dropout(drop_prob))\n",
    "\n",
    "  # Pool and flatten.\n",
    "  model.add(tf.keras.layers.GlobalAveragePooling2D(name=\"global_pool\"))\n",
    "  model.add(tf.keras.layers.Flatten(name=\"flat\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upzB3OXzXFmT"
   },
   "outputs": [],
   "source": [
    "def _xception_model(\n",
    "    input_shape: Tuple[int],\n",
    "    unfreeze_first=False,\n",
    "    unfreeze_last=False\n",
    ") -> tf.keras.Model:\n",
    "  \"\"\"Prepare exception model.\"\"\"\n",
    "  model = tf.keras.applications.xception.Xception(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "  model.trainable = False\n",
    "\n",
    "  if unfreeze_first:\n",
    "    model.layers[1].trainable = True\n",
    "    model.layers[4].trainable = True\n",
    "\n",
    "  if unfreeze_last:\n",
    "    model.layers[126].trainable = True\n",
    "    model.layers[129].trainable = True\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qek99g1I6BF"
   },
   "outputs": [],
   "source": [
    "def define_model(\n",
    "    custom_layers=3,\n",
    "    drop_prob=0.0,\n",
    "    embed=\"custom\",\n",
    "    extra_nodes=0,\n",
    "    input_shape=(224, 224, 3),\n",
    "    lr=1e-4,\n",
    "    l2=0.0,\n",
    "    unfreeze_first=False,\n",
    "    unfreeze_last=False\n",
    ") -> tf.keras.Model:\n",
    "  \"\"\"Define model.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    custom_layers: Layers for custom model.\n",
    "    drop_prob: Dropout probability.\n",
    "    embed: Embedding from [\"custom\", \"mobile\", \"resnet\", \"xception\"].\n",
    "    extra_nodes: Nodes in extra layer between embedder and output.\n",
    "    input_shape: Shape of inputs (excluding batch dimension).\n",
    "    lr: Learning rate.\n",
    "    l2: L2 weight decay penalty.\n",
    "    unfreeze_first: Unfreeze first few layers of pretrained model?\n",
    "    unfreeze_last: Unfreeze last few layers of pretrained model?\n",
    "  \n",
    "  \"\"\"\n",
    "  # Embedding module.\n",
    "  if embed == \"mobile\":\n",
    "    preprocessor = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    embedder = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "    embedder.trainable = False\n",
    "  # Resnet50.\n",
    "  elif embed == \"resnet\":\n",
    "    preprocessor = tf.keras.applications.resnet.preprocess_input\n",
    "    embedder = tf.keras.applications.resnet50.ResNet50(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "    embedder.trainable = False\n",
    "  # Xception.\n",
    "  elif embed == \"xception\":\n",
    "    preprocessor = tf.keras.applications.xception.preprocess_input\n",
    "    embedder = _xception_model(input_shape, unfreeze_first, unfreeze_last)\n",
    "  # Basic (default).\n",
    "  else:\n",
    "    preprocessor = tf.identity\n",
    "    embedder = _custom_model(\n",
    "        conv_blocks=custom_layers, drop_prob=drop_prob, l2=l2)\n",
    "\n",
    "  # Model.\n",
    "  inputs = tf.keras.Input(shape=input_shape, name=\"input\")\n",
    "  h = preprocessor(inputs)\n",
    "  h = embedder(h, training=False)\n",
    "  if (extra_nodes > 0):\n",
    "    h = tf.keras.layers.Dense(extra_nodes, activation=\"relu\")(h)\n",
    "  outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(h)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  # Compile.\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[\n",
    "          tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "          tf.keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFocHc3_HPrQ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cEm7NBmKHsh"
   },
   "outputs": [],
   "source": [
    "def train_and_eval(\n",
    "    pos_dir=POS_DIR,\n",
    "    neg_dir=NEG_DIR,\n",
    "    custom_layers=3,\n",
    "    drop_prob=0.0,\n",
    "    embed=\"custom\",\n",
    "    extra_nodes=0,\n",
    "    lr=1e-4,\n",
    "    l2=0.0,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=10,\n",
    "    unfreeze_first=False,\n",
    "    unfreeze_last=False,\n",
    ") -> Dict[str, float]:\n",
    "  \"\"\"Train and evaluate a model given the input data.\n",
    "  \n",
    "  Args\n",
    "  ----\n",
    "    pos_dir: Directory containing positive examples.\n",
    "    neg_dir: Directory containing negative examples.\n",
    "    custom_layers: Layers for custom model.\n",
    "    drop_prob: Dropout probability.\n",
    "    embed: Embedding from [\"custom\", \"mobile\", \"resnet\", \"xception\"].\n",
    "    extra_nodes: Nodes in extra layer between embedder and output.\n",
    "    lr: Learning rate.\n",
    "    l2: L2 penalty.\n",
    "    max_epochs: Maximum training epochs.\n",
    "    patience: Patience for early stopping.\n",
    "    unfreeze_first: Unfreeze first few layers of pretrained model?\n",
    "    unfreeze_last: Unfreeze last few layers of pretrained model?\n",
    "  \n",
    "  \"\"\"\n",
    "  # Load data.\n",
    "  pos = load_data(pos_dir)\n",
    "  neg = load_data(neg_dir)\n",
    "  data = prep_data_for_model(pos, neg)\n",
    "\n",
    "  # Prepare model.\n",
    "  model = define_model(\n",
    "      custom_layers=custom_layers,\n",
    "      drop_prob=drop_prob,\n",
    "      embed=embed, \n",
    "      extra_nodes=extra_nodes,\n",
    "      lr=lr,\n",
    "      l2=l2,\n",
    "      unfreeze_first=unfreeze_first,\n",
    "      unfreeze_last=unfreeze_last\n",
    "    )\n",
    "  print(model.summary())\n",
    "\n",
    "  # Prepare generators.\n",
    "  gen = prep_generators(data)\n",
    "\n",
    "  # Callbacks.\n",
    "  callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"final\"),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=patience, restore_best_weights=True)\n",
    "  ]\n",
    "\n",
    "  # Train model.\n",
    "  hist = model.fit(\n",
    "    x=gen[\"train_gen\"],\n",
    "    validation_data=gen[\"val_gen\"],\n",
    "    validation_freq=10,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    "  )\n",
    "\n",
    "  # Evaluate model.\n",
    "  train_eval = model.evaluate(\n",
    "    x=data[\"x_train\"],\n",
    "    y=data[\"y_train\"]\n",
    "  )\n",
    "  val_eval = model.evaluate(\n",
    "    x=data[\"x_val\"],\n",
    "    y=data[\"y_val\"]\n",
    "  )\n",
    "\n",
    "  # Output.\n",
    "  return {\n",
    "      \"train_loss\": train_eval[0],\n",
    "      \"train_acc\":  train_eval[1],\n",
    "      \"train_auc\":  train_eval[2],\n",
    "      \"val_loss\": val_eval[0],\n",
    "      \"val_acc\":  val_eval[1],\n",
    "      \"val_auc\":  val_eval[2]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8kpDaJPg5RD"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGcquZ-zjTvD"
   },
   "outputs": [],
   "source": [
    "def _fold_key(current_fold, length, total_folds) -> np.ndarray:\n",
    "  \"\"\"Key to select observations in current fold.\"\"\"\n",
    "  obs_per_fold = np.ceil(length / total_folds)\n",
    "  idx0 = int((current_fold) * obs_per_fold)\n",
    "  idx1 = int((current_fold + 1) * obs_per_fold)\n",
    "  key = np.zeros((length,), dtype=bool)\n",
    "  key[idx0:idx1] = True\n",
    "  return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybUTh-Y0g7Aw"
   },
   "outputs": [],
   "source": [
    "def model_eval(\n",
    "    Model: Callable,\n",
    "    pos_dir=POS_DIR,\n",
    "    neg_dir=NEG_DIR,\n",
    "    folds=5,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    ") -> Dict:\n",
    "  \"\"\"Evaluate model performance via cross validation.\n",
    "  \n",
    "  Args\n",
    "  ----\n",
    "    model: *Function* that returns the model to train. \n",
    "      Should require no arguments.\n",
    "    pos_dir: Directory containing positive examples.\n",
    "    neg_dir: Directory containing negative examples.\n",
    "    folds: Cross validation folds.\n",
    "  \n",
    "  \"\"\"\n",
    "  # Load data.\n",
    "  pos = load_data(pos_dir)\n",
    "  neg = load_data(neg_dir)\n",
    "\n",
    "  n_pos = pos.shape[0]\n",
    "  n_neg = neg.shape[0]\n",
    "\n",
    "  # Model evaluation.\n",
    "  evals = {\"fold\": [], \"acc\": [], \"auc\": []}\n",
    "\n",
    "  # Model predictions.\n",
    "  pos_yhat = np.zeros((n_pos, ))\n",
    "  neg_yhat = np.zeros((n_neg, ))\n",
    "\n",
    "  # Model embeddings.\n",
    "  model = Model()\n",
    "  embed_shape = model.layers[-2].output_shape[1]\n",
    "  pos_embed = np.zeros((n_pos, embed_shape))\n",
    "  neg_embed = np.zeros((n_neg, embed_shape))\n",
    "\n",
    "  # Loop over folds.\n",
    "  for k in range(folds):\n",
    "    pos_key = _fold_key(k, n_pos, folds)\n",
    "    neg_key = _fold_key(k, n_neg, folds)\n",
    "\n",
    "    pos_eval = pos[pos_key, :]\n",
    "    neg_eval = neg[neg_key, :]\n",
    "    pos_train = pos[~pos_key, :]\n",
    "    neg_train = neg[~neg_key, :]\n",
    "\n",
    "    data = prep_data_for_model(pos_train, neg_train)\n",
    "    \n",
    "    # Train model.\n",
    "    model = Model()\n",
    "    gen = prep_generators(data)\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(\n",
    "          patience=10, restore_best_weights=True)\n",
    "    ]\n",
    "    print(f\"\\n\\nStarting training for fold {k}.\\n\\n\")\n",
    "    hist = model.fit(\n",
    "      x=gen[\"train_gen\"],\n",
    "      validation_data=gen[\"val_gen\"],\n",
    "      validation_freq=10,\n",
    "      epochs=max_epochs,\n",
    "      callbacks=callbacks,\n",
    "      verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate model.\n",
    "    x = np.concatenate((pos_eval, neg_eval), axis=0)\n",
    "    y_pos = np.ones(pos_eval.shape[0])\n",
    "    y_neg = np.zeros(neg_eval.shape[0])\n",
    "    y = np.concatenate((y_pos, y_neg), axis=0)\n",
    "    fold_eval = model.evaluate(x, y)\n",
    "    \n",
    "    evals[\"fold\"].append(k)\n",
    "    evals[\"acc\"].append(fold_eval[1])\n",
    "    evals[\"auc\"].append(fold_eval[2])\n",
    "\n",
    "    # Model predictions.\n",
    "    pos_yhat[pos_key] = np.squeeze(model.predict(pos_eval))\n",
    "    neg_yhat[neg_key] = np.squeeze(model.predict(neg_eval))\n",
    "\n",
    "    # Model embeddings.\n",
    "    embedder = tf.keras.Sequential(model.layers[:-1])\n",
    "    pos_embed[pos_key] = embedder(pos_eval)\n",
    "    neg_embed[neg_key] = embedder(neg_eval)\n",
    "  \n",
    "  # Output.\n",
    "  return {\n",
    "      \"eval\": pd.DataFrame(evals, index=evals[\"fold\"]),\n",
    "      \"pos_yhat\": pos_yhat,\n",
    "      \"neg_yhat\": neg_yhat,\n",
    "      \"pos_embed\": pos_embed,\n",
    "      \"neg_embed\": neg_embed\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb2juHkPPT2m"
   },
   "source": [
    "# Depth of max projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSexdL6VQRE3"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  depths = [1, 3, 5, 7] \n",
    "\n",
    "  results = {\n",
    "    \"depth\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for d in depths:\n",
    "    results[\"depth\"].append(d)\n",
    "  \n",
    "  print(f\"\\n\\nStarting depth {d}.\\n\\n\")\n",
    "  out = train_and_eval(\n",
    "    pos_dir=f\"data/pos_d{d}\",\n",
    "    neg_dir=f\"data/neg_d{d}\",\n",
    "  )\n",
    "\n",
    "  for key in out:\n",
    "    results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/depth_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X38H1g8DWwBp"
   },
   "source": [
    "# Embedding Experiment\n",
    "* Vary the model used to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m6nzH9wW2MK"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  embedder = [\"custom\", \"mobile\", \"resnet\", \"xception\"]\n",
    "\n",
    "  results = {\n",
    "    \"embedder\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for e in embedder:\n",
    "    results[\"embedder\"].append(e)\n",
    "    \n",
    "    print(f\"\\n\\nStarting model with {e} embedding.\\n\\n\")\n",
    "    out = train_and_eval(\n",
    "      embed=e,\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "  \n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/embed_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P0g1OQiZTb9"
   },
   "source": [
    "## Extra fully connected layer between embedder and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFjAllgjQyE4"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  embedder = [\"custom\", \"mobile\", \"resnet\", \"xception\"]\n",
    "\n",
    "  results = {\n",
    "    \"embedder\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for e in embedder:\n",
    "    results[\"embedder\"].append(e)\n",
    "    \n",
    "    print(f\"\\n\\nStarting model with {e} embedding.\\n\\n\")\n",
    "    out = train_and_eval(\n",
    "      embed=e,\n",
    "      extra_nodes=32\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/embed_extra_layer_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcRiUMRDZbp1"
   },
   "source": [
    "## Xception fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqUyuZmlRP-z"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  unfreeze_first = [False, False, True, True]\n",
    "  unfreeze_last  = [False, True, False, True]\n",
    "\n",
    "  results = {\n",
    "    \"unfreeze_first\": [],\n",
    "    \"unfreeze_last\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for (uf, ul) in zip(unfreeze_first, unfreeze_last):\n",
    "    results[\"unfreeze_first\"].append(uf)\n",
    "    results[\"unfreeze_last\"].append(ul)\n",
    "    \n",
    "    print(f\"\"\"\\n\\n\n",
    "            Starting model with first unfrozen ({uf})\n",
    "            and last unfrozen ({ul}).\\n\\n\"\"\")\n",
    "    out = train_and_eval(\n",
    "      embed=\"xception\",\n",
    "      extra_nodes=32,\n",
    "      unfreeze_first=uf,\n",
    "      unfreeze_last=ul\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/xception_fine_tuning_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVriethD__w6"
   },
   "source": [
    "## Xception extra layer nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-LcyVLDaZHd"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  nodes = [0, 32, 64, 128]\n",
    "\n",
    "  results = {\n",
    "    \"nodes\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for n in nodes:\n",
    "    results[\"nodes\"].append(n)\n",
    "    \n",
    "    print(f\"\"\"\\n\\nStarting model with {n} nodes.\\n\\n\"\"\")\n",
    "    out = train_and_eval(\n",
    "      embed=\"xception\",\n",
    "      extra_nodes=n\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/xception_extra_nodes_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSrMS7KfAZwU"
   },
   "source": [
    "# Custom Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlQLWN6g592T"
   },
   "source": [
    "## Convolutional blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6OOm88kBGTv"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  layers = [1, 2, 3, 4]\n",
    "\n",
    "  results = {\n",
    "    \"layers\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for l in layers:\n",
    "    results[\"layers\"].append(l)\n",
    "    \n",
    "    print(f\"\"\"\\n\\nStarting model with {l} layers.\\n\\n\"\"\")\n",
    "    out = train_and_eval(\n",
    "      custom_layers=l,\n",
    "      embed=\"custom\",\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/custom_layers_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yebmU21pAkSC"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjHLJvg17Ah0"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  drop = [0.000, 0.125, 0.250, 0.375, 0.500]\n",
    "\n",
    "  results = {\n",
    "    \"drop_prob\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for d in drop:\n",
    "    results[\"drop_prob\"].append(d)\n",
    "    \n",
    "    print(f\"\"\"\\n\\nStarting dropout {d}.\\n\\n\"\"\")\n",
    "    out = train_and_eval(\n",
    "      custom_layers=2,\n",
    "      drop_prob=d,\n",
    "      embed=\"custom\",\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/custom_dropout_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "603ja_1GjWvi"
   },
   "source": [
    "## Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1TjbGqHBndo"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  decay = [0.0, 1.0e-4, 1.0e-3, 1.0e-2, 1.0e-1]\n",
    "\n",
    "  results = {\n",
    "    \"l2\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"train_auc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_auc\": []\n",
    "  }\n",
    "\n",
    "  # Run experiment.\n",
    "  for l2 in decay:\n",
    "    results[\"l2\"].append(l2)\n",
    "    \n",
    "    print(f\"\"\"\\n\\nStarting weight decay {l2}.\\n\\n\"\"\")\n",
    "    out = train_and_eval(\n",
    "      custom_layers=2,\n",
    "      drop_prob=0.375,\n",
    "      l2=l2,\n",
    "      embed=\"custom\",\n",
    "    )\n",
    "\n",
    "    for key in out:\n",
    "      results[key].append(out[key])\n",
    "\n",
    "  # Save results.\n",
    "  out = pd.DataFrame(results)\n",
    "  out.to_csv(\"results/custom_weight_decay_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0DBs7ycwoWh"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "159wa6dynXyU"
   },
   "outputs": [],
   "source": [
    "def Model() -> tf.keras.Model:\n",
    "  \"\"\"Final model.\"\"\"\n",
    "  return define_model(custom_layers=2, drop_prob=0.375,\n",
    "      embed=\"custom\", l2=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T-nmZT8w83c"
   },
   "outputs": [],
   "source": [
    "# Evaluate model.\n",
    "results = model_eval(Model)\n",
    "\n",
    "# Save results.\n",
    "results[\"eval\"].to_csv(\"results/final_eval.tsv\", sep=\"\\t\")\n",
    "np.save(\"pos_yhat.npy\", results[\"pos_yhat\"])\n",
    "np.save(\"neg_yhat.npy\", results[\"neg_yhat\"])\n",
    "np.save(\"pos_embed.npy\", results[\"pos_embed\"])\n",
    "np.save(\"neg_embed.npy\", results[\"neg_embed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DjMxk7XeMJp"
   },
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRl3gBmdfN48",
    "outputId": "c0e6047e-d972-4d2d-f92f-521bcb094448"
   },
   "outputs": [],
   "source": [
    "# Embeddings.\n",
    "x1 = results[\"pos_embed\"]\n",
    "x0 = results[\"neg_embed\"]\n",
    "x = np.concatenate((x1, x0), axis=0)\n",
    "\n",
    "# Labels.\n",
    "y1 = np.ones_like(results[\"pos_yhat\"])\n",
    "y0 = np.zeros_like(results[\"neg_yhat\"])\n",
    "y = np.concatenate((y1, y0), axis=0)\n",
    "\n",
    "# Predictions.\n",
    "yhat1 = results[\"pos_yhat\"]\n",
    "yhat0 = results[\"neg_yhat\"]\n",
    "yhat = np.concatenate((yhat1, yhat0), axis=0)\n",
    "\n",
    "# Fit t-SNE.\n",
    "tsne = TSNE(n_components=2)\n",
    "x_tsne = tsne.fit_transform(x)\n",
    "\n",
    "# Output.\n",
    "df = pd.DataFrame(x_tsne, columns=[\"tsne0\", \"tsne1\"])\n",
    "df[\"y\"] = y\n",
    "df[\"yhat\"] = yhat\n",
    "df.to_csv(\"results/eval_tsne.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "experiments.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
