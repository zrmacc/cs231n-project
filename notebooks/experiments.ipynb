{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow4sfyhNHvO1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o55_qqceH96-",
    "outputId": "ebf7a3a1-6639-4813-b05f-7529fb010e46"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/cs231n\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW0Q0y1qcDaB"
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOvJujhtdYx_"
   },
   "source": [
    "# Notes\n",
    "* `load_data` will recast the image shape to (224, 224, 3). This size is compatible with all candidate embedding architectures (MobileNetV2, ResNet50, Xception).\n",
    "\n",
    "* Each embedding architecture has a corresponding `preprocess_input` function through which the image should pass first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpTBf2cy2U31"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8DkIZQo2V8z"
   },
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    dir: str,\n",
    "    target_shape=[224, 224]\n",
    "  ) -> np.ndarray:\n",
    "  \"\"\"Load data.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    dir: Directory from which to load images.\n",
    "    target_shape: Shape for output\n",
    "\n",
    "  \"\"\"\n",
    "  orig_dir = os.getcwd()\n",
    "  os.chdir(dir)\n",
    "  files = os.listdir()\n",
    "  arrays = []\n",
    "  for in_file in files:\n",
    "    arrays.append(np.load(in_file, allow_pickle=True))\n",
    "  out = np.stack(arrays)\n",
    "  if target_shape != [256, 256]:\n",
    "    out = tf.image.resize(out, target_shape)\n",
    "    out = out.numpy()\n",
    "  os.chdir(orig_dir)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvW3aell6wCZ"
   },
   "source": [
    "## Training validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zA2CAdj8bc8"
   },
   "outputs": [],
   "source": [
    "def gen_sampling_weights(y: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Inverse probability sampling weights.\n",
    "  \n",
    "  Prepares a weight vector with weight inversely proportional to the\n",
    "  frequency of the class in the input vector.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    y: Binary [0, 1] labels.\n",
    "  \n",
    "  \"\"\"\n",
    "  n1 = sum(y == 1)\n",
    "  n0 = sum(y == 0)\n",
    "  p1 = n1 / (n1 + n0)\n",
    "  p0 = 1.0 - p1\n",
    "  w1 = 1.0 / p1\n",
    "  w0 = 1.0 / p0\n",
    "  out = np.ones_like(y)\n",
    "  out[y == 1] = w1\n",
    "  out[y == 0] = w0\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r987Hj8GDwro"
   },
   "outputs": [],
   "source": [
    "def prep_data_for_model(\n",
    "    pos: np.ndarray,\n",
    "    neg: np.ndarray,\n",
    "    train_prop = 0.6,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "  \"\"\"Prepare data for modeling.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    pos: Positive example images.\n",
    "    neg: Negative example images.\n",
    "    train_prop: Proportion of data for training. Remaining is allocated to\n",
    "    validation.\n",
    "  \n",
    "  \"\"\"\n",
    "  n1 = pos.shape[0]\n",
    "  n0 = neg.shape[0]\n",
    "  cut1 = int(train_prop * n1)\n",
    "  cut0 = int(train_prop * n0)\n",
    "\n",
    "  # Split into training and validation sets.\n",
    "  pos_train = pos[:cut1, :]\n",
    "  pos_val = pos[cut1:, :]\n",
    "  neg_train = neg[:cut0, :]\n",
    "  neg_val = neg[cut0:, :]\n",
    "  x_train = np.concatenate((pos_train, neg_train), axis=0)\n",
    "  x_val = np.concatenate((pos_val, neg_val), axis=0)\n",
    "\n",
    "  # Prepare labels.\n",
    "  y1_train = np.ones(pos_train.shape[0])\n",
    "  y0_train = np.zeros(neg_train.shape[0])\n",
    "  y_train = np.concatenate((y1_train, y0_train), axis=0)\n",
    "\n",
    "  y1_val = np.ones(pos_val.shape[0])\n",
    "  y0_val = np.zeros(neg_val.shape[0])\n",
    "  y_val = np.concatenate((y1_val, y0_val), axis=0)\n",
    "\n",
    "  # Prepare weights.\n",
    "  w_train = gen_sampling_weights(y_train)\n",
    "  w_val = gen_sampling_weights(y_val)\n",
    "\n",
    "  # Output.\n",
    "  return {\n",
    "      \"x_train\": x_train,\n",
    "      \"y_train\": y_train,\n",
    "      \"w_train\": w_train,\n",
    "      \"x_val\": x_val,\n",
    "      \"y_val\": y_val,\n",
    "      \"w_val\": w_val\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyQ4V1drIui_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfI79m9OIaA3"
   },
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjaZ9xYuGDgg"
   },
   "outputs": [],
   "source": [
    "def prep_generators(\n",
    "    data: Dict[str, np.ndarray],\n",
    "    train_batch=64,\n",
    "    val_batch=32,\n",
    ") -> Dict:\n",
    "  \"\"\"Prepare data generators\n",
    "\n",
    "  Generators apply random relections and rotations.\n",
    "\n",
    "  \"\"\"\n",
    "  transformer = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "  )\n",
    "\n",
    "  train_generator = transformer.flow(\n",
    "    x=data[\"x_train\"],\n",
    "    y=data[\"y_train\"],\n",
    "    batch_size=train_batch,\n",
    "    sample_weight=data[\"w_train\"]\n",
    "  )\n",
    "\n",
    "  val_generator = transformer.flow(\n",
    "    x=data[\"x_val\"],\n",
    "    y=data[\"y_val\"],\n",
    "    batch_size=val_batch,\n",
    "    sample_weight=data[\"w_val\"]\n",
    "  )\n",
    "\n",
    "  return {\n",
    "      \"train_gen\": train_generator,\n",
    "      \"val_gen\": val_generator\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT-Jt9eF9ZSY"
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qek99g1I6BF"
   },
   "outputs": [],
   "source": [
    "def define_mode(\n",
    "    embed=\"resnet\",\n",
    "    input_shape=(224, 224, 3),\n",
    "    lr=1e-4,\n",
    ") -> tf.keras.Model:\n",
    "  \"\"\"Define model.\n",
    "\n",
    "  Defines a model of the form:\n",
    "  Image -> Preprocessor -> Embedder -> Dense(1) -> Sigmoid.\n",
    "\n",
    "  Args\n",
    "  ----\n",
    "    lr: Learning rate.\n",
    "  \n",
    "  \"\"\"\n",
    "  # Embedding module.\n",
    "  if embed == \"mobile\":\n",
    "    preprocessor = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    embedder = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "  # Xception.\n",
    "  elif embed == \"xception\":\n",
    "    preprocessor = tf.keras.applications.xception.preprocess_input\n",
    "    embedder = tf.keras.applications.xception.Xception(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "  # Resnet50 (default).\n",
    "  else:\n",
    "    preprocessor = tf.keras.applications.resnet.preprocess_input\n",
    "    embedder = tf.keras.applications.resnet50.ResNet50(\n",
    "      include_top=False,\n",
    "      input_shape=input_shape,\n",
    "      pooling=\"avg\"\n",
    "    )\n",
    "  embedder.trainable = False\n",
    "\n",
    "  # Model.\n",
    "  inputs = tf.keras.Input(shape=input_shape, name=\"input\")\n",
    "  h = preprocessor(inputs)\n",
    "  h = embedder(h, training=False)\n",
    "  outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(h)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  # Compile.\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[\n",
    "          tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "          tf.keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFocHc3_HPrQ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cEm7NBmKHsh"
   },
   "outputs": [],
   "source": [
    "def train_and_eval(\n",
    "    pos_dir: str,\n",
    "    neg_dir: str,\n",
    "    lr=1e-4,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    embed=\"resnet\",\n",
    "    patience=10,\n",
    ") -> Dict[str, float]:\n",
    "  \"\"\"Train and evaluate a model given the input data.\n",
    "  \n",
    "  Args\n",
    "  ----\n",
    "    pos_dir: Directory containing positive examples.\n",
    "    neg_dir: Directory containing negative examples.\n",
    "    max_epochs: Maximum training epochs.\n",
    "    patience: Patience for early stopping.\n",
    "  \n",
    "  \"\"\"\n",
    "  # Load data.\n",
    "  pos = load_data(pos_dir)\n",
    "  neg = load_data(neg_dir)\n",
    "  data = prep_data_for_model(pos, neg)\n",
    "\n",
    "  # Prepare model.\n",
    "  model = define_mode(embed=embed, lr=lr)\n",
    "\n",
    "  # Prepare generators.\n",
    "  gen = prep_generators(data)\n",
    "\n",
    "  # Callbacks.\n",
    "  callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"final\"),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=patience, restore_best_weights=True)\n",
    "  ]\n",
    "\n",
    "  # Train model.\n",
    "  hist = model.fit(\n",
    "    x=gen[\"train_gen\"],\n",
    "    validation_data=gen[\"val_gen\"],\n",
    "    validation_freq=10,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    "  )\n",
    "\n",
    "  # Evaluate model.\n",
    "  train_eval = model.evaluate(\n",
    "    x=data[\"x_train\"],\n",
    "    y=data[\"y_train\"]\n",
    "  )\n",
    "  val_eval = model.evaluate(\n",
    "    x=data[\"x_val\"],\n",
    "    y=data[\"y_val\"]\n",
    "  )\n",
    "\n",
    "  # Output.\n",
    "  return {\n",
    "      \"train_loss\": train_eval[0],\n",
    "      \"train_acc\":  train_eval[1],\n",
    "      \"train_auc\":  train_eval[2],\n",
    "      \"val_loss\": val_eval[0],\n",
    "      \"val_acc\":  val_eval[1],\n",
    "      \"val_auc\":  val_eval[2]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb2juHkPPT2m"
   },
   "source": [
    "# Depth Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVra3FxnPU50"
   },
   "outputs": [],
   "source": [
    "# Projection depths.\n",
    "depths = [1, 3, 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f8RWb3hPYSf"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "  \"depth\": [],\n",
    "  \"train_loss\": [],\n",
    "  \"train_acc\": [],\n",
    "  \"train_auc\": [],\n",
    "  \"val_loss\": [],\n",
    "  \"val_acc\": [],\n",
    "  \"val_auc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPJ43S6tNSmL",
    "outputId": "5421abc9-8ad1-4691-ad54-02f57e9878c6"
   },
   "outputs": [],
   "source": [
    "# Run experiment.\n",
    "for d in depths:\n",
    "  results[\"depth\"].append(d)\n",
    "  \n",
    "  print(f\"\\n\\nStarting depth {d}.\\n\\n\")\n",
    "  out = train_and_eval(\n",
    "    pos_dir=f\"data/pos_d{d}\",\n",
    "    neg_dir=f\"data/neg_d{d}\",\n",
    "  )\n",
    "\n",
    "  for key in out:\n",
    "    results[key].append(out[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYzidl_0Q2wS"
   },
   "outputs": [],
   "source": [
    "# Save results.\n",
    "out = pd.DataFrame(results)\n",
    "out.to_csv(\"results/depth_experiment.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X38H1g8DWwBp"
   },
   "source": [
    "# Embedding Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m6nzH9wW2MK"
   },
   "outputs": [],
   "source": [
    "# Embedders\n",
    "embedder = [\"mobile\", \"resnet\", \"xception\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G79WUAxXrpH"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "  \"embedder\": [],\n",
    "  \"train_loss\": [],\n",
    "  \"train_acc\": [],\n",
    "  \"train_auc\": [],\n",
    "  \"val_loss\": [],\n",
    "  \"val_acc\": [],\n",
    "  \"val_auc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chblFnKVbyH6",
    "outputId": "3fb93d2a-d7e0-41e4-efb5-faaae6b0668c"
   },
   "outputs": [],
   "source": [
    "# Run experiment.\n",
    "d = 1\n",
    "for e in embedder:\n",
    "  results[\"embedder\"].append(e)\n",
    "  \n",
    "  print(f\"\\n\\nStarting model with {e} embedding.\\n\\n\")\n",
    "  out = train_and_eval(\n",
    "    pos_dir=f\"data/pos_d{d}\",\n",
    "    neg_dir=f\"data/neg_d{d}\",\n",
    "    embed=e,\n",
    "  )\n",
    "\n",
    "  for key in out:\n",
    "    results[key].append(out[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTvOBsusb6Kp"
   },
   "outputs": [],
   "source": [
    "# Save results.\n",
    "out = pd.DataFrame(results)\n",
    "out.to_csv(\"results/embed_experiment.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "experiments.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
